# -*- coding: utf-8 -*-
"""Emotion_Recognition_from Speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/180xtaFHh5CR7aT7BXXzQTx0YESdoUmsA
"""

pip install librosa numpy pandas scikit-learn tensorflow matplotlib

"""Defining Categories of Emotions

"""

# Original 8 emotion labels
emotion_dict = {
    "01": "neutral",
    "02": "neutral",  # Mapping 'calm' as 'neutral'
    "03": "happy",
    "04": "sad",
    "05": "angry",
    "06": "sad",  # Mapping 'fearful' as 'sad'
    "07": "angry",  # Mapping 'disgust' as 'angry'
    "08": "happy"   # Mapping 'surprised' as 'happy'
}

# Define the final emotion categories
final_emotions = ["angry", "sad", "neutral", "happy"]

"""# **Dataset Uploading**"""

from google.colab import drive
drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/audio_speech_actors_01-24/Actor_01'

"""# **Using CNN Model**

Feature Extraction using MFCC
"""

import os
import librosa
import numpy as np

def extract_features(dataset_path):
    features = []
    labels = []

    for file in os.listdir(dataset_path):
        if file.endswith(".wav"):
            file_path = os.path.join(dataset_path, file)

            # Load audio file
            y, sr = librosa.load(file_path, sr=None)

            # Extract MFCC features
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
            mfccs = np.mean(mfccs, axis=1)  # Take mean across time

            features.append(mfccs)

            # Extract emotion from filename and map to 4-class label
            filename_parts = file.split("-")  # Split filename
            emotion_code = filename_parts[2]  # 3rd element is emotion code
            emotion_label = emotion_dict[emotion_code]  # Convert to 4 emotions

            labels.append(emotion_label)

    return np.array(features), np.array(labels)

# Extract features and labels
X, y = extract_features(dataset_path)

print("Feature Shape:", X.shape)  # Expected: (num_samples, 40)
print("Labels Shape:", y.shape)    # Expected: (num_samples,)

"""Encode Labels"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Encode labels to numerical values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Converts "happy" → 0, "sad" → 1, etc.

# Convert labels to one-hot encoding
y_one_hot = to_categorical(y_encoded, num_classes=4)

"""Split Dataset"""

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)

print("Updated Labels:", label_encoder.classes_)  # Should print ['angry' 'happy' 'neutral' 'sad']
print("Training Data Shape:", X_train.shape)
print("Testing Data Shape:", X_test.shape)

"""Normalize MFCC Features"""

from sklearn.preprocessing import StandardScaler

# Apply Standard Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Reshape Data for CNN"""

# Reshape for CNN (Adding a channel dimension)
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

print("CNN Input Shape:", X_train_cnn.shape)

"""Perform Data Augmentation"""

import random

def augment_data(X, y):
    augmented_X = []
    augmented_y = []

    for i in range(len(X)):
        augmented_X.append(X[i])
        augmented_y.append(y[i])

        # Add small noise for augmentation
        if random.random() > 0.5:
            noise = np.random.normal(0, 0.01, X[i].shape)
            augmented_X.append(X[i] + noise)
            augmented_y.append(y[i])

    return np.array(augmented_X), np.array(augmented_y)

# Apply augmentation to training data
X_train, y_train = augment_data(X_train, y_train)

print("Augmented Training Set Shape:", X_train.shape)

print(y_train.shape, y_test.shape)

""" Build and Train the CNN Model"""

import tensorflow as tf
from tensorflow.keras import layers, models

# Assuming X_train and X_test have shape (num_samples, 40)
# We need to reshape for CNN: (samples, features, 1)
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)


# Build CNN model
cnn_model = models.Sequential([
    layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Conv1D(128, kernel_size=3, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Conv1D(256, kernel_size=3, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(4, activation='softmax')  # num_classes defined earlier
])

# Compile the CNN model
cnn_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

cnn_model.summary()

# Train the CNN model
history_cnn = cnn_model.fit(X_train_cnn, y_train,
                            epochs=100,
                            batch_size=64,
                            validation_data=(X_test_cnn, y_test))

"""Evaluate the CNN Model"""

# Evaluate on test set
test_loss, test_acc1 = cnn_model.evaluate(X_test_cnn, y_test)
print(f"Test Accuracy (CNN): {test_acc1:.2f}")

# Plot training and validation accuracy
import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title("CNN Model Accuracy")
plt.show()

"""# Testing the model with an Audio File"""

import librosa
import numpy as np

def extract_mfcc(file_path, n_mfcc=40):
    # Load audio file
    y, sr = librosa.load(file_path, sr=None)

    # Extract MFCC features
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)

    # Take the mean across time frames to get a fixed-size input
    mfcc_mean = np.mean(mfcc, axis=1)

    return mfcc_mean

# Example: Extract MFCC features from an audio file
sample_audio = "/content/drive/MyDrive/audio_speech_actors_01-24/Actor_01/03-01-02-01-01-01-02.wav"  # Audio File with Neutral Emotion
mfcc_features = extract_mfcc(sample_audio)

print("MFCC Shape:", mfcc_features.shape)

# Reshape for CNN input format
mfcc_features = mfcc_features.reshape(1, 40, 1)  # (1 sample, 40 features, 1 channel)

"""Making the Prediction"""

# Make prediction
y_pred = cnn_model.predict(mfcc_features)

# Get the predicted label (index of highest probability)
predicted_label = np.argmax(y_pred)

# Map label index to emotion class
emotion_classes = label_encoder.classes_  # ['angry', 'happy', 'neutral', 'sad']
predicted_emotion = emotion_classes[predicted_label]

print(f"Predicted Emotion: {predicted_emotion}")

"""# Hyperparameter Tunning

First
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np

# Encode labels to numerical values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Converts "happy" → 0, "sad" → 1, etc.

# Convert labels to one-hot encoding (only 4 classes)
y_one_hot = to_categorical(y_encoded, num_classes=4)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)

print("Updated Labels:", label_encoder.classes_)  # Should print ['angry' 'happy' 'neutral' 'sad']
print("Training Data Shape:", X_train.shape)
print("Testing Data Shape:", X_test.shape)

# Data Augmentation
import random

def augment_data(X, y):
    augmented_X = []
    augmented_y = []

    for i in range(len(X)):
        augmented_X.append(X[i])
        augmented_y.append(y[i])

        # Add small noise for augmentation
        if random.random() > 0.5:
            noise = np.random.normal(0, 0.01, X[i].shape)
            augmented_X.append(X[i] + noise)
            augmented_y.append(y[i])

    return np.array(augmented_X), np.array(augmented_y)

# Apply augmentation to training data
X_train, y_train = augment_data(X_train, y_train)

print("Augmented Training Set Shape:", X_train.shape)

# Reshape for CNN (Adding a channel dimension)
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

print("CNN Input Shape:", X_train_cnn.shape)

import tensorflow as tf
from tensorflow.keras import layers, models

# Build CNN model
cnn_model = models.Sequential([
    layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Conv1D(128, kernel_size=3, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Conv1D(256, kernel_size=3, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(4, activation='softmax')  # Matching 4 emotion classes
])

# Compile the CNN model
cnn_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

cnn_model.summary()

# Train the CNN model
history_cnn = cnn_model.fit(X_train_cnn, y_train,
                            epochs=100,
                            batch_size=64,
                            validation_data=(X_test_cnn, y_test))

# Evaluate on test set
test_loss, test_acc2 = cnn_model.evaluate(X_test_cnn, y_test)
print(f"Test Accuracy (CNN): {test_acc2:.2f}")

"""Second"""

import tensorflow as tf
from tensorflow.keras import layers, models

# Assuming X_train and X_test have shape (num_samples, 40)
# We need to reshape for CNN: (samples, features, 1)
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)


# Build CNN model
cnn_model = models.Sequential([
    layers.Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Conv1D(128, kernel_size=3, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Conv1D(256, kernel_size=3, activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),

    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(4, activation='softmax')
])

# Compile the CNN model
cnn_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

cnn_model.summary()

# Train the CNN model
history_cnn = cnn_model.fit(X_train_cnn, y_train,
                            epochs=100,
                            batch_size=20,
                            validation_data=(X_test_cnn, y_test))

# Evaluate on test set
test_loss, test_acc3 = cnn_model.evaluate(X_test_cnn, y_test)
print(f"Test Accuracy (CNN): {test_acc3:.2f}")

# Plot training and validation accuracy
import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title("CNN Model Accuracy")
plt.show()

"""Third"""

from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.optimizers import Adam

# Build an improved CNN model
cnn_model = models.Sequential([
    layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1),
                  kernel_regularizer=regularizers.l2(0.0005)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.3),  # Reduced Dropout

    layers.Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.1),  # Slightly lower dropout

    layers.Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),
    layers.BatchNormalization(),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.1),

    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),
    layers.Dropout(0.3),  # Lower dropout in Dense layer
    layers.Dense(4, activation='softmax')  # 4 emotion classes
])

# Change optimizer to Adam (better adaptation)
cnn_model.compile(optimizer=Adam(learning_rate=0.0005),  # Lower learning rate for stability
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

cnn_model.summary()

# Train the CNN model
history_cnn = cnn_model.fit(X_train_cnn, y_train,
                            epochs=80,  # Reduce total epochs for better monitoring
                            batch_size=64,  # Increased batch size for better updates
                            validation_data=(X_test_cnn, y_test))

import matplotlib.pyplot as plt
# Evaluate on test set
test_loss, test_acc4 = cnn_model.evaluate(X_test_cnn, y_test)
print(f"Test Accuracy (CNN): {test_acc4:.2f}")

# Plot Accuracy
plt.figure(figsize=(8,4))
plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title("CNN Model Accuracy")
plt.show()

import librosa
import numpy as np

def extract_mfcc(file_path, n_mfcc=40):
    # Load audio file
    y, sr = librosa.load(file_path, sr=None)

    # Extract MFCC features
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)

    # Take the mean across time frames to get a fixed-size input
    mfcc_mean = np.mean(mfcc, axis=1)

    return mfcc_mean

# Example: Extract MFCC features from an audio file
sample_audio = "/content/drive/MyDrive/audio_speech_actors_01-24/Actor_01/03-01-03-01-01-01-07.wav"  # Audio file with happy emotion
mfcc_features = extract_mfcc(sample_audio)

print("MFCC Shape:", mfcc_features.shape)  # Should be (40,)

# Reshape for CNN input format
mfcc_features = mfcc_features.reshape(1, 40, 1)  # (1 sample, 40 features, 1 channel)

# Make prediction
y_pred = cnn_model.predict(mfcc_features)

# Get the predicted label (index of highest probability)
predicted_label = np.argmax(y_pred)

# Map label index to emotion class
emotion_classes = label_encoder.classes_  # ['angry', 'happy', 'neutral', 'sad']
predicted_emotion = emotion_classes[predicted_label]

print(f"Predicted Emotion: {predicted_emotion}")

"""## Using SVM"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Initialize SVM model
svm_model = SVC(kernel='rbf', C=10, gamma='scale')  # Radial Basis Function (RBF) kernel

# Train the model
svm_model.fit(X_train, y_train.argmax(axis=1))  # Convert one-hot labels to single-label encoding

# Predict on test set
y_pred_svm = svm_model.predict(X_test)

# Evaluate SVM model
svm_acc = accuracy_score(y_test.argmax(axis=1), y_pred_svm)
print(f"SVM Accuracy: {svm_acc:.2f}")

"""# **Using Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Initialize Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train.argmax(axis=1))

# Predict on test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate Random Forest model
rf_acc = accuracy_score(y_test.argmax(axis=1), y_pred_rf)
print(f"Random Forest Accuracy: {rf_acc:.2f}")

"""Hypterparameter Tunning"""

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train.argmax(axis=1))

print("Best Random Forest Parameters:", grid_search.best_params_)
print("Best Random Forest Accuracy:", grid_search.best_score_)

"""# **Using XGB Classifier**"""

from xgboost import XGBClassifier

# Initialize XGBoost model
xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, use_label_encoder=False, eval_metric='mlogloss')

# Train the model
xgb_model.fit(X_train, y_train.argmax(axis=1))

# Predict on test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate XGBoost model
xgb_acc = accuracy_score(y_test.argmax(axis=1), y_pred_xgb)
print(f"XGBoost Accuracy: {xgb_acc:.2f}")

"""Hypterparameter Tunning"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

grid_search = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train.argmax(axis=1))

print("Best XGBoost Parameters:", grid_search.best_params_)
print("Best XGBoost Accuracy:", grid_search.best_score_)

"""Performance with tuned parameter"""

from xgboost import XGBClassifier

# Initialize XGBoost model
xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.2, max_depth=5, use_label_encoder=False, eval_metric='mlogloss')

# Train the model
xgb_model.fit(X_train, y_train.argmax(axis=1))

# Predict on test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate XGBoost model
xgb_acc = accuracy_score(y_test.argmax(axis=1), y_pred_xgb)
print(f"XGBoost Accuracy: {xgb_acc:.2f}")

"""# **Model Performance Comparison**"""

print("\nModel Performance Comparison:")
print(f"CNN Accuracy: {test_acc2:.2f}")
print(f"SVM Accuracy: {svm_acc:.2f}")
print(f"Random Forest Accuracy: {rf_acc:.2f}")
print(f"XGBoost Accuracy: {xgb_acc:.2f}")

"""#**Saving the Model with best Performance**"""

import joblib
from tensorflow.keras.models import save_model, load_model

# Store model accuracies in a dictionary
model_performance = {
    "CNN": test_acc2,
    "SVM": svm_acc,
    "Random_Forest": rf_acc,
    "XGBoost": xgb_acc
}

# Identify the best model
best_model_name = max(model_performance, key=model_performance.get)
best_accuracy = model_performance[best_model_name]

print(f"\n Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}")

# Save the best model
if best_model_name == "CNN":
    save_model(cnn_model, "best_model.h5")  # Save Keras model
elif best_model_name == "MLP":
    save_model(mlp_model, "best_model.h5")
else:  # For SVM, Random Forest, and XGBoost
    joblib.dump(eval(best_model_name.lower() + "_model"), "best_model.pkl")

print(f" Best model saved as {'best_model.h5' if best_model_name in ['CNN', 'MLP'] else 'best_model.pkl'}")

if best_model_name in ["CNN", "MLP"]:
    best_model = load_model("best_model.h5")  # Load Keras model
else:
    best_model = joblib.load("best_model.pkl")  # Load ML model

print(f"\n Loaded best model: {best_model_name}")

"""# **Testing the model**"""

emotion_labels = ["Angry", "Sad", "Neutral", "Happy"]

sample_audio = "/content/drive/MyDrive/audio_speech_actors_01-24/Actor_01/03-01-05-01-01-01-17.wav"  # Audio File with angy emotion

# Extract MFCC Features
mfcc_features = extract_mfcc(sample_audio)

# Reshape for model input
mfcc_features = np.expand_dims(mfcc_features, axis=0)  # Shape: (1, 40)

# Convert to correct dtype
mfcc_features = mfcc_features.astype(np.float32)

# 🔹 Make Predictions
predictions = best_model.predict(mfcc_features)

# 🔹 Convert prediction to class label
if best_model_name in ["CNN", "MLP"]:
    predicted_label = np.argmax(predictions)  # Get index of highest probability
else:
    predicted_label = int(predictions[0])  # ML models output a direct class

predicted_emotion = emotion_labels[predicted_label]  # Map index to emotion

# 🔹 Print the Predicted Emotion
print(f"\n Predicted Emotion: {predicted_emotion}")

