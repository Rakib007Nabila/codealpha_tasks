# -*- coding: utf-8 -*-
"""Parkinson's_Detection using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZssaF60wqhEkHE7idTJGgmQKMbQxjCD

# **Parkinson's Detection using ML**

# **Uploading Dataset**

Dataset Link:https://archive.ics.uci.edu/ml/datasets/Parkinsons
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('parkinsons.data')

# Display the first few rows
print(df.head(20))

"""Understanding the Dataset"""

# Check dataset structure
print(df.info())

# Check for missing values
print(df.isnull().sum())

# Display statistical summary
print(df.describe())

"""Selecting Features & Target Variable"""

# Drop 'name' column as it's not useful
df = df.drop(columns=['name'])

# Define features (X) and target (y)
X = df.drop(columns=['status'])  # Features
y = df['status']                 # Target (0 or Healthy 1 for PD)

# Display shape of X and y
print(f"Features Shape: {X.shape}, Target Shape: {y.shape}")

"""Splitting Data into Train & Test Sets"""

from sklearn.model_selection import train_test_split

# Split dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shape of training and testing sets
print(f"Training Data Shape: {X_train.shape}, Testing Data Shape: {X_test.shape}")

"""Feature Scaling (Normalization)"""

from sklearn.preprocessing import StandardScaler

# Initialize scaler
scaler = StandardScaler()

# Fit and transform the training data
X_train = scaler.fit_transform(X_train)

# Transform the testing data
X_test = scaler.transform(X_test)

"""# Train Using Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize model
model1 = RandomForestClassifier(n_estimators=100, random_state=42)

# Train model
model1.fit(X_train, y_train)

# Make predictions
y_pred = model1.predict(X_test)

"""Evaluation of Model"""

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

# Display Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""**Predict Parkinson’s for a New Patient**"""

# testing for a new patient
new_patient = [11.992, 5.302, 14.997, 0.00784, 0.00007, 0.0370, 0.00554, 0.0140,
               0.1908, 0.64, 0.355, 211.093, 0.02184, 0.01908, 0.00143, 222.105, 0.426,
               0.141, 211.94, 0.00784, 0.7, 0.00370]

# Scale the new patient data
new_patient_scaled = scaler.transform([new_patient])

# Predict Parkinson’s status
predicted_status = model1.predict(new_patient_scaled)
print("Predicted Status:", "Parkinson's" if predicted_status[0] == 1 else "Healthy")

"""# Use different Models: Logistic Regression, SVM, Neural Network"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(kernel='linear'),
    "Neural Network": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions
    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy
    print(f"{name} Accuracy: {acc:.4f}")

    # Display Classification Report
    print(f"\n Classification Report for {name}:")
    print(classification_report(y_test, y_pred))

    # Compute Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot Confusion Matrix
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"Confusion Matrix - {name}")
    plt.show()

# Initialize model
NN =MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)

# Train model
NN.fit(X_train, y_train)

# Make predictions
y_pred = NN.predict(X_test)
acc = accuracy_score(y_test, y_pred)  # Calculate accuracy
print(f"{name} Accuracy: {acc:.4f}")

"""Feature Engineering"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest Model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importance scores
feature_importances = rf.feature_importances_

# Sort and plot feature importances
indices = np.argsort(feature_importances)[::-1]  # Sort in descending order
plt.figure(figsize=(10,6))
sns.barplot(x=X.columns[indices], y=feature_importances[indices])
plt.xticks(rotation=90)
plt.xlabel("Feature Name")
plt.ylabel("Importance Score")
plt.title("Feature Importance Ranking")
plt.show()

"""Select only the top 10 important features and check acuuracy for Logistic Regression"""

# Select top 10 important features
important_features = X.columns[indices[:10]]
X_train_selected = X_train[:, indices[:10]]
X_test_selected = X_test[:, indices[:10]]

# Retrain Logistic Regression with selected features
log_reg = LogisticRegression()
log_reg.fit(X_train_selected, y_train)
y_pred = log_reg.predict(X_test_selected)
print(f"Logistic Regression Accuracy (Selected Features): {accuracy_score(y_test, y_pred):.4f}")

"""Check acuuracy for Random Forest Classifier for selected 10 important Features"""

# Train Random Forest on selected features
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_selected, y_train)

# Predict on test set
y_pred_rf = rf.predict(X_test_selected)

# Print accuracy
print(f"Random Forest Accuracy (Selected Features): {accuracy_score(y_test, y_pred_rf):.4f}")

"""Check acuuracy for SVM for selected 10 important Features"""

# Train SVM on selected features
svm = SVC(kernel='linear')  # Linear kernel for better interpretability
svm.fit(X_train_selected, y_train)

# Predict on test set
y_pred_svm = svm.predict(X_test_selected)

# Print accuracy
print(f"SVM Accuracy (Selected Features): {accuracy_score(y_test, y_pred_svm):.4f}")

"""Check acuuracy for Neural Network for selected 10 important Features"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define Neural Network architecture
nn_model = Sequential([
    Dense(32, activation='relu', input_shape=(10,)),  # 10 selected features
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary classification (change activation for multi-class)
])

# Compile the model
nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the Neural Network
nn_model.fit(X_train_selected, y_train, epochs=20, batch_size=32, verbose=0)

# Evaluate on test data
loss, acc_nn = nn_model.evaluate(X_test_selected, y_test, verbose=0)
print(f"Neural Network Accuracy (Selected Features): {acc_nn:.4f}")

"""# **Hyperparameter Tuning Using GridSearchCV**

For SVM
"""

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit model
grid_search.fit(X_train, y_train)

# Get best parameters
print("Best Parameters:", grid_search.best_params_)

# Evaluate best model
best_svm = grid_search.best_estimator_
y_pred = best_svm.predict(X_test)
print(f"Tuned SVM Accuracy: {accuracy_score(y_test, y_pred):.4f}")

"""For Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Define parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],  # Number of trees
    'max_depth': [10, 20, 30],        # Maximum depth of trees
    'min_samples_split': [2, 5, 10]   # Minimum samples to split a node
}

# Initialize GridSearchCV
grid_search_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1  # Use all available processors
)

# Fit GridSearchCV
grid_search_rf.fit(X_train, y_train)

# Get best parameters
print("Best Random Forest Parameters:", grid_search_rf.best_params_)

# Evaluate best model
best_rf = grid_search_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test)
print(f"Tuned Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")

"""#**Save the Best Model Based on Accuracy**"""

import joblib
from tensorflow.keras.models import save_model

# Dictionary to store model names and their corresponding accuracy
model_performance = {}

# using the best accuracy for different models
model_performance["random_forest"] = 0.9487
model_performance["svm"] = 0.9231
model_performance["Neural Network"] = 0.9487
model_performance["logistic_regression"] = 0.9231

# Identify the best model
best_model_name = max(model_performance, key=model_performance.get)
best_accuracy = model_performance[best_model_name]

print(f"Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}")

# Save the best model
if best_model_name == "random_forest":
    joblib.dump(best_rf, "best_model.pkl")
elif best_model_name == "svm":
    joblib.dump(best_svm, "best_model.pkl")
elif best_model_name == "Neural Network":
    joblib.dump(NN,"best_model.pkl")
elif best_model_name == "logistic_regression":
    joblib.dump(log_reg, "best_model.pkl")
print(f"Best model saved as best_model.{('h5' if best_model_name == 'mlp' else 'pkl')}")

import joblib
from tensorflow.keras.models import load_model

# Dictionary to store model names and their corresponding accuracy
model_performance = {
    "random_forest": 0.9487,
    "svm": 0.9231,
    "Neural Network": 0.9487,
    "logistic_regression": 0.9231
}

# Identify the best model dynamically
best_model_name = max(model_performance, key=model_performance.get)
best_accuracy = model_performance[best_model_name]

print(f"Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}")

# Load the best model properly
if best_model_name == "Neural Network":
    best_model = load_model("best_model.h5")  # Load Keras model
else:
    best_model = joblib.load("best_model.pkl")  # Load ML models

print(f" Loaded best model: {best_model_name}")

"""# **Testing the model with another Dataset**"""

# Load test data
file_path = "/content/parkinsons_updrs.data"
test_data = pd.read_csv(file_path)

# Print column names to check for non-numeric columns
print("\n Test Data Columns:", test_data.columns)

# Ensure 'status' column is removed before prediction
if "status" in test_data.columns:
    X_test_new = test_data.drop(columns=["status"]).values  # Drop target column
else:
    X_test_new = test_data.values  # Use all data if 'status' is missing

"""**Choosing the best model**"""

# Normalize if required
if best_model_name == "random_forest":
    scaler = StandardScaler()
    X_test_new = scaler.fit_transform(X_test_new)

# Get last 5 samples for prediction
X_last_5 = X_test_new[-5:]


# Make predictions
predictions = best_model.predict(X_last_5)

# Convert predictions to class labels for Neural Networks
if best_model_name == "random_forest":
    predictions = predictions.astype(int)

"""**Making Prediction**"""

# Print Predictions
print("\n Predictions for Last 5 Data Points:")
for i, pred in enumerate(predictions):
    predicted_label = "Healthy" if pred == 0 else "Parkinson's"
    print(f"Sample {len(X_test_new)-5+i+1}: Predicted Class -> {predicted_label}")

